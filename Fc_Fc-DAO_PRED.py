#Prediction of the Fc-site glycosylation distributon of Fc-DAO protein for quadruple Î²-1,4-galacosyltransferase knockouts
#using of triple-knockouts of b4GalT for training and validation
#The present file trains a network of 2 hidden layers and tunes the number of neurons in each layer
#The script is developed for Python 3.7

#HL1 = 23
#HL2 = 16

import numpy as np

#######################
#Defining the sigmoid activation function and its derivative
def sigmoid(x):
    return 1/(1+np.exp(-x))

def sigmoid_der(x):
    return sigmoid(x)*(1-sigmoid(x))
########################

max_nodes = 30
epochs = 20000

error = np.zeros([max_nodes, max_nodes]) #recording the difference between the output of the network and the experimental data

### Define the training set
for i in range(1, max_nodes+1):
    print(i)
    for j in range(1, max_nodes+1):

        np.random.seed(0)
        feature_set = np.array([[0.182481751824818,	0.182481751824818,	0.182481751824818,	0.182481751824818],
        [0.178789289688708,	0.186174213960927,	0.178789289688708,	0.186174213960927],
        [0.186174213960927,	0.178789289688708,	0.186174213960927,	0.178789289688708],
        [0.176184218720569,	0.184226127904914,	0.188779284929066,	0.188779284929066],
        [0.184226127904914,	0.176184218720569,	0.180737375744721,	0.180737375744721],
        [0.180737375744721,	0.180737375744721,	0.184226127904914,	0.176184218720569],
        [0.188779284929066,	0.188779284929066,	0.176184218720569,	0.184226127904914],
        [0.174083281691501,	0.190880221958135,	0.185157535097343,	0.179805968552292],
        [0.183342941704071,	0.181620561945565,	0.177625110078364,	0.187338393571272],
        [0.179805968552292,	0.177625110078364,	0.190880221958135,	0.183342941704071],
        [0.187338393571272,	0.185157535097343,	0.181620561945565,	0.174083281691501],
        [0.177625110078364,	0.179805968552292,	0.179805968552292,	0.185157535097343],
        [0.185157535097343,	0.187338393571272,	0.187338393571272,	0.177625110078364],
        [0.181620561945565,	0.183342941704071,	0.174083281691501,	0.181620561945565],
        [0.190880221958135,	0.174083281691501,	0.183342941704071,	0.190880221958135],
        [0.172284314594407,	0.182911016858027,	0.181183200131412,	0.178231267730942],
        [0.172080291970803,	0.000565693430656934,	3.64963503649635E-05,	0],
        [0.168598300176451,	0.000577140063278874,	3.57578579377415E-05,	0],
        [0.175562283765154,	0.000554246798034994,	3.72348427921854E-05,	0],
        [0.166141718253497,	0.000571100996505234,	3.77558569858131E-05,	0],
        [0.173725238614334,	0.000546171078033765,	3.61474751489441E-05,	0],
        [0.170435345327272,	0.000560285864808634,	3.68452255809828E-05,	0],
        [0.178018865688109,	0.000585215783280104,	3.52368437441139E-05,	0],
        [0.164160534635085,	0.000591728688070219,	3.70315070194686E-05,	0],
        [0.172892394026939,	0.000563023742031251,	3.55250220156728E-05,	0],
        [0.169557028344811,	0.000550637841242928,	0.000038176044391627,	0],
        [0.176660105137709,	0.000573988358801763,	3.63241123891129E-05,	0],
        [0.167500478803897,	0.000557398502512105,	3.59611937104584E-05,	0],
        [0.174603555596794,	0.000580749020070943,	3.74676787142544E-05,	0],
        [0.171268189914668,	0.000568363119282621,	3.48166563383001E-05,	0],
        [0.180000049306521,	0.000539658173243652,	3.66685883408142E-05,	0],
        [0.162464108662526,	0.000567024152259885,	3.62366400262823E-05,	0],
        [0.193886861313869,	0.000821167883211679,	9.12408759124088E-05,	0.00010948905109489],
        [0.189963620294252,	0.000837783962824171,	8.93946448443538E-05,	0.000111704528376556],
        [0.197810102333485,	0.000804551803599184,	9.30871069804635E-05,	0.000107273573813225],
        [0.187195732390605,	0.000829017575572114,	9.43896424645328E-05,	0.000113267570957439],
        [0.195740260898971,	0.000792828984242562,	9.03686878723603E-05,	0.000108442425446832],
        [0.192033461728766,	0.000813318190851243,	9.21130639524571E-05,	0.000105710531232342],
        [0.200577990237132,	0.000849506782180795,	8.80921093602847E-05,	0.000110535676742949],
        [0.18496348679722,	0.000858960998811608,	9.25787675486715E-05,	0.000107883581131375],
        [0.194801875560576,	0.000817292528755041,	8.88125550391819E-05,	0.000112403036142763],
        [0.19104384158681,	0.000799312995352637,	9.54401109790675E-05,	0.000110005765022443],
        [0.199047043169476,	0.000833208907938044,	9.08102809727824E-05,	0.0001044499690149],
        [0.188726679458262,	0.000809126858485314,	0.000089902984276146,	0.000111094521058406],
        [0.196729881040927,	0.000843022771070724,	0.000093669196785636,	0.000106575066047018],
        [0.192971847067163,	0.00082504323766832,	8.70416408457504E-05,	0.000108972337167339],
        [0.202810235830518,	0.000783374767611753,	9.16714708520356E-05,	0.000114528133174881],
        [0.183052084256557,	0.000823099575861123,	9.05916000657058E-05,	0.000106938760638565],
        [3.64963503649635E-05,	0.5,	1.82481751824818E-05,	0],
        [3.57578579377415E-05,	0.51011734625294,	1.78789289688708E-05,	0],
        [3.72348427921854E-05,	0.489882653747059,	1.86174213960927E-05,	0],
        [3.52368437441139E-05,	0.504779590459465,	1.88779284929066E-05,	0],
        [3.68452255809828E-05,	0.48274475929436,	1.80737375744721E-05,	0],
        [3.61474751489441E-05,	0.495220409540534,	1.84226127904914E-05,	0],
        [3.77558569858131E-05,	0.51725524070564,	1.76184218720569E-05,	0],
        [3.48166563383001E-05,	0.52301180816529,	1.85157535097343E-05,	0],
        [3.66685883408142E-05,	0.497640339730847,	1.77625110078364E-05,	0],
        [3.59611937104584E-05,	0.486692801614717,	1.90880221958135E-05,	0],
        [3.74676787142544E-05,	0.50733164616672,	1.81620561945565E-05,	0],
        [3.55250220156728E-05,	0.49266835383328,	1.79805968552292E-05,	0],
        [3.70315070194686E-05,	0.513307198385285,	1.87338393571272E-05,	0],
        [3.63241123891129E-05,	0.502359660269155,	1.74083281691501E-05,	0],
        [0.000038176044391627,	0.476988191834712,	1.83342941704071E-05,	0],
        [3.44568629188814E-05,	0.501176186190995,	1.81183200131412E-05,	0],
        [1.82481751824818E-05,	0.418266423357664,	1.82481751824818E-05,	1.82481751824818E-05],
        [1.78789289688708E-05,	0.426729915819841,	1.78789289688708E-05,	1.86174213960927E-05],
        [1.86174213960927E-05,	0.409802930895487,	1.86174213960927E-05,	1.78789289688708E-05],
        [1.76184218720569E-05,	0.422264707770854,	1.88779284929066E-05,	1.88779284929066E-05],
        [1.84226127904914E-05,	0.403831847729417,	1.80737375744721E-05,	1.80737375744721E-05],
        [1.80737375744721E-05,	0.414268138944474,	1.84226127904914E-05,	1.76184218720569E-05],
        [1.88779284929066E-05,	0.432700998985911,	1.76184218720569E-05,	1.84226127904914E-05],
        [1.74083281691501E-05,	0.437516556750241,	1.85157535097343E-05,	1.79805968552292E-05],
        [1.83342941704071E-05,	0.416292490035429,	1.77625110078364E-05,	1.87338393571272E-05],
        [1.79805968552292E-05,	0.407134514810618,	1.90880221958135E-05,	1.83342941704071E-05],
        [1.87338393571272E-05,	0.42439958619662,	1.81620561945565E-05,	1.74083281691501E-05],
        [1.77625110078364E-05,	0.412133260518708,	1.79805968552292E-05,	1.85157535097343E-05],
        [1.85157535097343E-05,	0.429398331904712,	1.87338393571272E-05,	1.77625110078364E-05],
        [1.81620561945565E-05,	0.420240356679902,	1.74083281691501E-05,	1.81620561945565E-05],
        [1.90880221958135E-05,	0.399016289965089,	1.83342941704071E-05,	1.90880221958135E-05],
        [1.72284314594407E-05,	0.419250341740285,	1.81183200131412E-05,	1.78231267730942E-05],
        [5.47445255474452E-05,	0.385547445255474,	0,	0],
        [5.36367869066123E-05,	0.393348879256647,	0,	0],
        [5.58522641882781E-05,	0.377746011254302,	0,	0],
        [5.28552656161708E-05,	0.389232963037503,	0,	0],
        [5.52678383714743E-05,	0.372242017312819,	0,	0],
        [5.42212127234162E-05,	0.381861927473446,	0,	0],
        [5.66337854787197E-05,	0.39885287319813,	0,	0],
        [5.22249845074502E-05,	0.403291732953148,	0,	0],
        [5.50028825112213E-05,	0.383727923278589,	0,	0],
        [5.39417905656876E-05,	0.375286332573567,	0,	0],
        [5.62015180713816E-05,	0.391200840153666,	0,	0],
        [5.32875330235092E-05,	0.379894050357282,	0,	0],
        [5.55472605292029E-05,	0.395808557937383,	0,	0],
        [5.44861685836694E-05,	0.387366967232362,	0,	0],
        [5.72640665874405E-05,	0.367803157557803,	0,	0],
        [5.16852943783221E-05,	0.38645439641764,	0,	0],
        [3.64963503649635E-05,	0.000291970802919708,	0.283868613138686,	0],
        [3.57578579377415E-05,	0.000297878742337483,	0.278124619039754,	0],
        [3.72348427921854E-05,	0.000286062863501932,	0.289612607237618,	0],
        [3.52368437441139E-05,	0.000294761804647863,	0.293665055635655,	0],
        [3.68452255809828E-05,	0.000281894749952911,	0.281155061708487,	0],
        [3.61474751489441E-05,	0.000289179801191553,	0.286582164568885,	0],
        [3.77558569858131E-05,	0.000302046855886505,	0.274072170641718,	0],
        [3.48166563383001E-05,	0.000305408355133016,	0.288031061597427,	0],
        [3.66685883408142E-05,	0.000290592899112904,	0.276313621237903,	0],
        [3.59611937104584E-05,	0.000284200176125382,	0.296933273278075,	0],
        [3.74676787142544E-05,	0.000296252056155749,	0.282528946162521,	0],
        [3.55250220156728E-05,	0.000287689549683667,	0.279706164679945,	0],
        [3.70315070194686E-05,	0.000299741429714035,	0.291423605039471,	0],
        [3.63241123891129E-05,	0.000293348706726514,	0.270803952999299,	0],
        [0.000038176044391627,	0.000278533250706401,	0.285208280114853,	0],
        [3.44568629188814E-05,	0.000292657626972844,	0.281848586124424,	0],
        [5.47445255474452E-05,	0.000602189781021898,	0.127700729927007,	0],
        [5.36367869066123E-05,	0.000614374906071059,	0.125116744924158,	0],
        [5.58522641882781E-05,	0.000590004655972735,	0.130284714929857,	0],
        [5.28552656161708E-05,	0.000607946222086217,	0.13210774359336,	0],
        [5.52678383714743E-05,	0.000581407921777879,	0.126480015546155,	0],
        [5.42212127234162E-05,	0.000596433339957578,	0.128921444307859,	0],
        [5.66337854787197E-05,	0.000622971640265917,	0.123293716260654,	0],
        [5.22249845074502E-05,	0.000629904732461846,	0.129573243061121,	0],
        [5.50028825112213E-05,	0.000599347854420364,	0.124302052032839,	0],
        [5.39417905656876E-05,	0.000586162863258601,	0.133577979326303,	0],
        [5.62015180713816E-05,	0.000611019865821232,	0.127098069249506,	0],
        [5.32875330235092E-05,	0.000593359696222564,	0.125828216792894,	0],
        [5.55472605292029E-05,	0.000618216698785197,	0.131099407821176,	0],
        [5.44861685836694E-05,	0.000605031707623435,	0.121823480527712,	0],
        [5.72640665874405E-05,	0.000574474829581952,	0.128303390604509,	0],
        [5.16852943783221E-05,	0.00060360635563149,	0.126792003451962,	0],
        [0.000164233576642336,	0.00295620437956204,	0.30742700729927,	0],
        [0.000160910360719837,	0.00301602226616702,	0.301206316338566,	0],
        [0.000167556792564834,	0.00289638649295706,	0.313647698259974,	0],
        [0.000158565796848512,	0.00298446327205961,	0.318036461319997,	0],
        [0.000165803515114423,	0.00285418434327322,	0.304488256917131,	0],
        [0.000162663638170249,	0.00292794548706447,	0.310365757681409,	0],
        [0.000169901356436159,	0.00305822441585086,	0.296817553278543,	0],
        [0.000156674953522351,	0.00309225959572179,	0.311934899378494,	0],
        [0.000165008647533664,	0.00294225310351815,	0.29924502294902,	0],
        [0.000161825371697063,	0.00287752678326949,	0.32157590993287,	0],
        [0.000168604554214145,	0.00299955206857696,	0.305976160709693,	0],
        [0.000159862599070527,	0.00291285669054713,	0.302919115220046,	0],
        [0.000166641781587609,	0.0030348819758546,	0.315608991649522,	0],
        [0.000163458505751008,	0.00297015565560595,	0.293278104665671,	0],
        [0.000171792199762322,	0.00282014916340231,	0.308877853888849,	0],
        [0.000155055883134966,	0.00296315847310004,	0.305239337261389,	0]])

        
        labels = np.array([[0,	0.013,	0.114,	0.431,	0.347,	0.036,	0.017,	0],
        [0,	0.0132630510025764,	0.111693245054329,	0.439721152470034,	0.339978561700459,	0.0367284489302117,	0.0166560102274,	0],
        [0,	0.0127369489974235,	0.11630675494567,	0.422278847529965,	0.35402143829954,	0.0352715510697883,	0.0173439897726,	0],
        [0,	0.0131242693519461,	0.117934194880886,	0.445874017488262,	0.350317035778869,	0.0347576226691939,	0.0168374939243782,	0],
        [0,	0.0125513637416534,	0.112910253375242,	0.426879993023941,	0.335024862950286,	0.0363441305130815,	0.0175866781839918,	0],
        [0,	0.0128757306480539,	0.115089746624758,	0.416125982511738,	0.358975137049714,	0.0372423773308061,	0.0164133218160082,	0],
        [0,	0.0134486362583466,	0.110065805119114,	0.435120006976059,	0.343682964221131,	0.0356558694869185,	0.0171625060756218,	0],
        [0,	0.0135983070122975,	0.115671615326012,	0.424680121004287,	0.337764804320614,	0.0343431498120993,	0.0169197715508488,	0],
        [0,	0.012938648833002,	0.110965958768155,	0.442470805008116,	0.352088162439704,	0.0361698955393792,	0.0177824014776199,	0],
        [0,	0.0126540128419826,	0.119246692261686,	0.433034027152012,	0.345362395773208,	0.0369581182837405,	0.0165475552549004,	0],
        [0,	0.0131906228003347,	0.113461997458633,	0.411163821361522,	0.362970194866711,	0.0354721214759962,	0.0172492759696685,	0],
        [0,	0.0128093771996653,	0.112328384673988,	0.437319878995713,	0.348637604226794,	0.0350418817162596,	0.0162175985223802,	0],
        [0,	0.0133459871580174,	0.117034041231845,	0.419529194991886,	0.33102980513329,	0.0365278785240038,	0.0170802284491513,	0],
        [0,	0.013061351166998,	0.108753307738314,	0.428965972847991,	0.356235195679388,	0.0376568501879009,	0.0167507240303315,	0],
        [0,	0.0124016929877025,	0.114538002541367,	0.45083617863848,	0.341911837560296,	0.035830104460621,	0.0174524447450997,	0],
        [0,	0.0130305808409659,	0.113188768786096,	0.420960866628357,	0.346183726783448,	0.035374537264764,	0.0179499932523851,	0],
        [0,	0.011,	0.101,	0.409,	0.377,	0.032,	0.021,	0.011],
        [0,	0.0112225816175647,	0.0989562960569059,	0.417275989234905,	0.369371520925283,	0.0326475101601882,	0.0205750714573765,	0.0112225816175647],
        [0,	0.0107774183824353,	0.103043703943094,	0.400724010765094,	0.384628479074717,	0.0313524898398118,	0.0214249285426235,	0.0107774183824353],
        [0,	0.0111051509901082,	0.104485558622539,	0.423114786897213,	0.380603811206437,	0.030895664594839,	0.0207992572007024,	0.0108948490098918],
        [0,	0.0106203847044759,	0.100034522727188,	0.405090295004157,	0.363989548507947,	0.0323058937894058,	0.0217247201096369,	0.0113796152955241],
        [0,	0.0108948490098918,	0.101965477272812,	0.394885213102786,	0.390010451492053,	0.033104335405161,	0.0202752798903631,	0.0111051509901082],
        [0,	0.0113796152955241,	0.0975144413774607,	0.412909704995842,	0.373396188793563,	0.0316941062105942,	0.0212007427992975,	0.0106203847044759],
        [0,	0.0115062597796364,	0.102480992525677,	0.403002713435623,	0.366966372417497,	0.0305272442774216,	0.0209008942686956,	0.0110519125259214],
        [0,	0.0109480874740786,	0.0983119459261728,	0.419885288279163,	0.382528061209707,	0.0321510182572259,	0.0219664959429422,	0.0104937402203637],
        [0,	0.0107072416355238,	0.105648385249389,	0.410930202100169,	0.375220816157059,	0.0328516606966582,	0.0204410976678181,	0.0108387037843322],
        [0,	0.0111612962156678,	0.100523348625631,	0.390176340920794,	0.394350903356629,	0.0315307746453299,	0.0213079291390022,	0.0112927583644763],
        [0,	0.0108387037843322,	0.0995190074743226,	0.414997286564377,	0.378779183842943,	0.0311483393033419,	0.0200335040570579,	0.0115062597796364],
        [0,	0.0112927583644763,	0.103688054073828,	0.398114711720839,	0.359649096643373,	0.0324692253546701,	0.0210991057313045,	0.0109480874740786],
        [0,	0.0110519125259214,	0.0963516147506118,	0.407069797899833,	0.387033627582505,	0.0334727557225786,	0.0206920708609978,	0.0107072416355238],
        [0,	0.0104937402203637,	0.101476651374369,	0.427823659079207,	0.371471938790293,	0.0318489817427742,	0.021558902332182,	0.0111612962156678],
        [0,	0.0110258760962019,	0.100281277608734,	0.399473304990715,	0.376113155611989,	0.0314440331242347,	0.0221735210764757,	0.0108088863864557],
        [0.016,	0.022,	0.065,	0.37,	0.461,	0.027,	0.013,	0],
        [0.0156762449199059,	0.0224451632351294,	0.0636847449871177,	0.377486836227176,	0.451671806754788,	0.0275463366976588,	0.0127369489974235,	0],
        [0.0163237550800941,	0.0215548367648706,	0.0663152550128822,	0.362513163772824,	0.470328193245211,	0.0264536633023412,	0.0132630510025764,	0],
        [0.0154478322974195,	0.0222103019802165,	0.0672431812917332,	0.382768878122174,	0.465406782403627,	0.0260682170018954,	0.0128757306480539,	0],
        [0.0161529468947029,	0.0212407694089518,	0.0643786532402695,	0.366463103059996,	0.4450906680694,	0.0272580978848111,	0.0134486362583466,	0],
        [0.0158470531052971,	0.0217896980197835,	0.0656213467597304,	0.357231121877826,	0.4769093319306,	0.0279317829981046,	0.0125513637416534,	0],
        [0.0165521677025805,	0.0227592305910482,	0.0627568187082668,	0.373536896940004,	0.456593217596373,	0.0267419021151889,	0.0131242693519461,	0],
        [0.0152636221387108,	0.0230125195592728,	0.0659531140016736,	0.364574581836627,	0.448730763088769,	0.0257573623590745,	0.012938648833002,	0],
        [0.016075509128613,	0.0218961749481573,	0.0632700642099132,	0.379847326805111,	0.467759777765716,	0.0271274216545344,	0.0135983070122975,	0],
        [0.015765387322665,	0.0214144832710476,	0.0679915350614877,	0.371746148599175,	0.458824393231841,	0.0277185887128054,	0.0126540128419826,	0],
        [0.0164258303483291,	0.0223225924313357,	0.0646932441650102,	0.352971261957687,	0.482216887128397,	0.0266040911069971,	0.0131906228003347,	0],
        [0.0155741696516709,	0.0216774075686643,	0.0640468859983264,	0.375425418163373,	0.463175606768161,	0.0262814112871947,	0.0124016929877025,	0],
        [0.016234612677335,	0.0225855167289525,	0.0667299357900871,	0.360152673194891,	0.439783112871604,	0.0273959088930029,	0.013061351166998,	0],
        [0.0159244908713871,	0.0221038250518428,	0.0620084649385126,	0.368253851400827,	0.473269236911233,	0.0282426376409257,	0.0128093771996653,	0],
        [0.0167363778612893,	0.0209874804407273,	0.0653067558349902,	0.387028738042315,	0.454240222234284,	0.0268725783454658,	0.0133459871580174,	0],
        [0.0151058887036376,	0.0220517521924038,	0.0645374558868088,	0.361381718451258,	0.459915556331901,	0.026530902948573,	0.0137264654282945,	0],
        [0.01,	0.025,	0,	0.039,	0.823,	0.058,	0,	0],
        [0.00979765307494118,	0.025505867312647,	0,	0.0397891530077293,	0.806346848067659,	0.059173612165341,	0,	0],
        [0.0102023469250588,	0.024494132687353,	0,	0.0382108469922706,	0.839653151932339,	0.0568263878346588,	0,	0],
        [0.0096548951858872,	0.0252389795229732,	0,	0.0403459087750399,	0.830867205896279,	0.0559983920781458,	0,	0],
        [0.0100955918091893,	0.024137237964718,	0,	0.0386271919441617,	0.794597873798517,	0.0585544324932979,	0,	0],
        [0.00990440819081069,	0.0247610204770267,	0,	0.0376540912249601,	0.851402126201483,	0.0600016079218542,	0,	0],
        [0.0103451048141128,	0.025862762035282,	0,	0.0393728080558383,	0.81513279410372,	0.057445567506702,	0,	0],
        [0.00953976383669424,	0.0261505904082645,	0,	0.0384281315989958,	0.801096351457824,	0.0553306302528266,	0,	0],
        [0.0100471932053831,	0.0248820169865424,	0,	0.0400379614740522,	0.835067889590421,	0.058273720591222,	0,	0],
        [0.0098533670766656,	0.0243346400807359,	0,	0.0391840535009941,	0.819115999196975,	0.0595436350126931,	0,	0],
        [0.0102661439677057,	0.025366582308336,	0,	0.0372050789631075,	0.860877436240067,	0.0571495290446605,	0,	0],
        [0.00973385603229434,	0.024633417691664,	0,	0.0395718684010042,	0.826884000803029,	0.0564563649873072,	0,	0],
        [0.0101466329233344,	0.0256653599192643,	0,	0.0379620385259479,	0.785122563759936,	0.0588504709553395,	0,	0],
        [0.00995280679461695,	0.0251179830134578,	0,	0.0388159464990061,	0.844903648542179,	0.0606693697471736,	0,	0],
        [0.0104602361633058,	0.0238494095917356,	0,	0.0407949210368926,	0.810932110409579,	0.0577262794087783,	0,	0],
        [0.0094411804397735,	0.0250588093095498,	0,	0.0380915865394569,	0.82106399752962,	0.0569923100376753,	0,	0],
        [0.011,	0.027,	0,	0.041,	0.817,	0.059,	0,	0],
        [0.0107774183824353,	0.0275463366976588,	0,	0.0418296223927411,	0.800468256222694,	0.0601938468578469,	0,	0],
        [0.0112225816175647,	0.0264536633023412,	0,	0.0401703776072588,	0.833531743777304,	0.057806153142153,	0,	0],
        [0.0106203847044759,	0.0272580978848111,	0,	0.0424149297378625,	0.824809850810766,	0.0569638815967345,	0,	0],
        [0.0111051509901082,	0.0260682170018954,	0,	0.0406080735823238,	0.788804936686984,	0.0595639916742169,	0,	0],
        [0.0108948490098918,	0.0267419021151889,	0,	0.0395850702621375,	0.845195063313016,	0.0610361184032655,	0,	0],
        [0.0113796152955241,	0.0279317829981046,	0,	0.0413919264176761,	0.809190149189233,	0.0584360083257831,	0,	0],
        [0.0104937402203637,	0.0282426376409257,	0,	0.040398805014329,	0.795256037838448,	0.056284606636496,	0,	0],
        [0.0110519125259214,	0.0268725783454658,	0,	0.0420911902675934,	0.828979909836421,	0.0592784399117603,	0,	0],
        [0.0108387037843322,	0.0262814112871947,	0,	0.0411934921420707,	0.813144315120205,	0.0605702494094636,	0,	0],
        [0.0112927583644763,	0.0273959088930029,	0,	0.0391130317304464,	0.854601294542084,	0.058134865752327,	0,	0],
        [0.0107072416355238,	0.0266040911069971,	0,	0.041601194985671,	0.820855684879799,	0.0574297505905366,	0,	0],
        [0.0111612962156678,	0.0277185887128054,	0,	0.0399088097324068,	0.77939870545792,	0.059865134247673,	0,	0],
        [0.0109480874740786,	0.0271274216545344,	0,	0.0408065078579295,	0.838743962161556,	0.0617153933635042,	0,	0],
        [0.0115062597796364,	0.0257573623590745,	0,	0.0428869682695538,	0.80502009016358,	0.05872156008824,	0,	0],
        [0.0103852984837509,	0.0270635140543137,	0,	0.040045001233788,	0.815078111763912,	0.0579749360728077,	0,	0],
        [0,	0.023,	0,	0.041,	0.824,	0.054,	0,	0],
        [0,	0.0234653979276352,	0,	0.0418296223927411,	0.807326613375153,	0.0550926733953175,	0,	0],
        [0,	0.0225346020723647,	0,	0.0401703776072588,	0.840673386624845,	0.0529073266046824,	0,	0],
        [0,	0.0232198611611354,	0,	0.0424149297378625,	0.831876765077198,	0.0521364340037909,	0,	0],
        [0,	0.0222062589275406,	0,	0.0406080735823238,	0.795563363317105,	0.0545161957696222,	0,	0],
        [0,	0.0227801388388646,	0,	0.0395850702621375,	0.852436636682895,	0.0558635659962091,	0,	0],
        [0,	0.0237937410724594,	0,	0.0413919264176761,	0.816123234922801,	0.0534838042303777,	0,	0],
        [0,	0.0240585431756033,	0,	0.040398805014329,	0.802069737061054,	0.0515147247181489,	0,	0],
        [0,	0.022891455627619,	0,	0.0420911902675934,	0.836082552882755,	0.0542548433090687,	0,	0],
        [0,	0.022387868874277,	0,	0.0411934921420707,	0.820111279876437,	0.0554371774256108,	0,	0],
        [0,	0.0233372557236691,	0,	0.0391130317304464,	0.861923459856398,	0.0532081822139942,	0,	0],
        [0,	0.0226627442763309,	0,	0.041601194985671,	0.827888720123568,	0.0525628225743894,	0,	0],
        [0,	0.0236121311257231,	0,	0.0399088097324068,	0.786076540143606,	0.0547918177860058,	0,	0],
        [0,	0.0231085443723811,	0,	0.0408065078579295,	0.84593026293895,	0.0564852752818513,	0,	0],
        [0,	0.0219414568243968,	0,	0.0428869682695538,	0.811917447117246,	0.0537451566909315,	0,	0],
        [0,	0.0230541045647858,	0,	0.040045001233788,	0.822061645157238,	0.053061805897146,	0,	0],
        [0,	0.02,	0,	0.022,	0.861,	0.065,	0,	0],
        [0,	0.0204046938501176,	0,	0.0224451632351294,	0.843577929752436,	0.0663152550128822,	0,	0],
        [0,	0.0195953061498824,	0,	0.0215548367648706,	0.878422070247563,	0.0636847449871177,	0,	0],
        [0,	0.0201911836183786,	0,	0.0227592305910482,	0.869230454771199,	0.0627568187082668,	0,	0],
        [0,	0.0193097903717744,	0,	0.0217896980197835,	0.831286475504888,	0.0656213467597304,	0,	0],
        [0,	0.0198088163816214,	0,	0.0212407694089518,	0.890713524495112,	0.0672431812917332,	0,	0],
        [0,	0.0206902096282256,	0,	0.0222103019802165,	0.8527695452288,	0.0643786532402695,	0,	0],
        [0,	0.0209204723266116,	0,	0.0216774075686643,	0.838085004380543,	0.0620084649385126,	0,	0],
        [0,	0.0199056135892339,	0,	0.0225855167289525,	0.873625094699092,	0.0653067558349902,	0,	0],
        [0,	0.0194677120645887,	0,	0.0221038250518428,	0.856936665016519,	0.0667299357900871,	0,	0],
        [0,	0.0202932658466688,	0,	0.0209874804407273,	0.900626333660629,	0.0640468859983264,	0,	0],
        [0,	0.0197067341533312,	0,	0.0223225924313357,	0.865063334983485,	0.0632700642099132,	0,	0],
        [0,	0.0205322879354114,	0,	0.0214144832710476,	0.821373666339374,	0.0659531140016736,	0,	0],
        [0,	0.0200943864107662,	0,	0.0218961749481573,	0.883914995619461,	0.0679915350614877,	0,	0],
        [0,	0.0190795276733885,	0,	0.0230125195592728,	0.848374905300908,	0.0646932441650102,	0,	0],
        [0,	0.0200470474476398,	0,	0.0214875616376424,	0.858974607379104,	0.0638706922836016,	0,	0],
        [0,	0.015,	0,	0.021,	0.869,	0.057,	0,	0],
        [0,	0.0153035203875882,	0,	0.0214249285426235,	0.851416052212389,	0.0581533774728352,	0,	0],
        [0,	0.0146964796124118,	0,	0.0205750714573765,	0.88658394778761,	0.0558466225271647,	0,	0],
        [0,	0.0151433877137839,	0,	0.0217247201096369,	0.87730692821855,	0.055032902559557,	0,	0],
        [0,	0.0144823427788308,	0,	0.0207992572007024,	0.839010391653598,	0.057544873312379,	0,	0],
        [0,	0.014856612286216,	0,	0.0202752798903631,	0.898989608346402,	0.058967097440443,	0,	0],
        [0,	0.0155176572211692,	0,	0.0212007427992975,	0.860693071781449,	0.0564551266876209,	0,	0],
        [0,	0.0156903542449587,	0,	0.0206920708609978,	0.845872089206378,	0.0543766538691572,	0,	0],
        [0,	0.0149292101919254,	0,	0.021558902332182,	0.881742401037759,	0.0572690012706837,	0,	0],
        [0,	0.0146007840484415,	0,	0.0210991057313045,	0.864898910452213,	0.0585170206159225,	0,	0],
        [0,	0.0152199493850016,	0,	0.0200335040570579,	0.908994522591274,	0.0561641923369939,	0,	0],
        [0,	0.0147800506149984,	0,	0.0213079291390022,	0.873101089547792,	0.0554829793840777,	0,	0],
        [0,	0.0153992159515586,	0,	0.0204410976678181,	0.82900547740873,	0.0578358076630061,	0,	0],
        [0,	0.0150707898080746,	0,	0.0209008942686956,	0.892127910793626,	0.0596233461308431,	0,	0],
        [0,	0.0143096457550414,	0,	0.0219664959429422,	0.856257598962241,	0.0567309987293166,	0,	0],
        [0,	0.0150352855857299,	0,	0.0205108542904768,	0.866955788400048,	0.056009684002543,	0,	0],
        [0,	0.015,	0,	0.021,	0.869,	0.057,	0,	0],
        [0,	0.0153035203875882,	0,	0.0214249285426235,	0.851416052212389,	0.0581533774728352,	0,	0],
        [0,	0.0146964796124118,	0,	0.0205750714573765,	0.88658394778761,	0.0558466225271647,	0,	0],
        [0,	0.0151433877137839,	0,	0.0217247201096369,	0.87730692821855,	0.055032902559557,	0,	0],
        [0,	0.0144823427788308,	0,	0.0207992572007024,	0.839010391653598,	0.057544873312379,	0,	0],
        [0,	0.014856612286216,	0,	0.0202752798903631,	0.898989608346402,	0.058967097440443,	0,	0],
        [0,	0.0155176572211692,	0,	0.0212007427992975,	0.860693071781449,	0.0564551266876209,	0,	0],
        [0,	0.0156903542449587,	0,	0.0206920708609978,	0.845872089206378,	0.0543766538691572,	0,	0],
        [0,	0.0149292101919254,	0,	0.021558902332182,	0.881742401037759,	0.0572690012706837,	0,	0],
        [0,	0.0146007840484415,	0,	0.0210991057313045,	0.864898910452213,	0.0585170206159225,	0,	0],
        [0,	0.0152199493850016,	0,	0.0200335040570579,	0.908994522591274,	0.0561641923369939,	0,	0],
        [0,	0.0147800506149984,	0,	0.0213079291390022,	0.873101089547792,	0.0554829793840777,	0,	0],
        [0,	0.0153992159515586,	0,	0.0204410976678181,	0.82900547740873,	0.0578358076630061,	0,	0],
        [0,	0.0150707898080746,	0,	0.0209008942686956,	0.892127910793626,	0.0596233461308431,	0,	0],
        [0,	0.0143096457550414,	0,	0.0219664959429422,	0.856257598962241,	0.0567309987293166,	0,	0],
        [0,	0.0150352855857299,	0,	0.0205108542904768,	0.866955788400048,	0.056009684002543,	0,	0]])

        
#ANN parameters
        wh1 = np.random.rand(len(feature_set[0]),i)
        wh2 = np.random.rand(i, j)
        wo  = np.random.rand(j, len(labels[0]))
        lr = 0.5

        #print (wh1) #it was silenced in order to save some time during optimization

        for epoch in range(epochs):
            #FEEDFORWARD
            zh1 = np.dot(feature_set, wh1)
            ah1 = sigmoid(zh1)

            zh2 = np.dot(ah1, wh2)
            ah2 = sigmoid(zh2)
            
            zo = np.dot(ah2, wo)
            ao = sigmoid(zo)

#ANN error
            error_output = ((1/2)*(np.power((ao - labels), 2)))

#Part1: from HL2 to the Output
            dcost_dao = ao - labels
            dao_dzo   = sigmoid_der(zo)
            dzo_dwo   = ah2
            dcost_dwo = np.dot(dzo_dwo.T, dcost_dao*dao_dzo)
#Part2: from HL1 to HL2
            dcost_dzo = dcost_dao*dao_dzo
            dzo_dah2  = wo
            dcost_dah2 = np.dot(dcost_dzo, dzo_dah2.T)
            dah2_dzh2 = sigmoid_der(zh2)
            dzh2_dwh2 = ah1
            dcost_dwh2 = np.dot(dzh2_dwh2.T, dah2_dzh2*dcost_dah2)
#Part2: from the Input to HL1
            dcost_dzh2 = dcost_dah2*dah2_dzh2
            dzh2_dah1  = wh2
            dcost_dah1 = np.dot(dcost_dzh2, dzh2_dah1.T)
            dah1_dzh1 = sigmoid_der(zh1)
            dzh1_dwh1 = feature_set
            dcost_dwh1 = np.dot(dzh1_dwh1.T, dah1_dzh1*dcost_dah1)
#update weights
            wo  -= lr*dcost_dwo
            wh2 -= lr*dcost_dwh2
            wh1 -= lr*dcost_dwh1

        #Validation set (tuning of the hyperparameters)
        validation_point = np.array([[1.82481751824818E-05,	0.00114963503649635,	0,	0.127919708029197],	
        [1.78789289688708E-05,	0.00117289754795384,	0,	0.13050812398661],	
        [1.86174213960927E-05,	0.00112637252503886,	0,	0.125331292071784],	
        [1.76184218720569E-05,	0.00116062460580096,	0,	0.132334278735275],	
        [1.84226127904914E-05,	0.00110996057793959,	0,	0.126696900397049],	
        [1.80737375744721E-05,	0.00113864546719174,	0,	0.123505137323119],	
        [1.88779284929066E-05,	0.00118930949505311,	0,	0.129142515661345],	
        [1.74083281691501E-05,	0.00120254539833625,	0,	0.126043983955157],	
        [1.83342941704071E-05,	0.00114420954025706,	0,	0.131324213893462],	
        [1.79805968552292E-05,	0.00111903819349369,	0,	0.128523402134554],	
        [1.87338393571272E-05,	0.00116649247111326,	0,	0.122032380465742],	
        [1.77625110078364E-05,	0.00113277760187944,	0,	0.129795432103237],	
        [1.85157535097343E-05,	0.00118023187949901,	0,	0.124515202164933],	
        [1.81620561945565E-05,	0.00115506053273565,	0,	0.127316013923841],	
        [1.90880221958135E-05,	0.00109672467465645,	0,	0.133807035592653],	
        [1.72284314594407E-05,	0.00115233940620557,	0,	0.12494011867939],	
        [1.82481751824818E-05,	0.00104014598540146,	0,	0.200729927007299],	
        [1.78789289688708E-05,	0.00106119301957728,	0,	0.20479163535702],	
        [1.86174213960927E-05,	0.00101909895122563,	0,	0.196668218657578],	
        [1.76184218720569E-05,	0.00105008892905801,	0,	0.207657213421972],	
        [1.84226127904914E-05,	0.00100425004670725,	0,	0.198811113319193],	
        [1.80737375744721E-05,	0.00103020304174491,	0,	0.193802640592626],	
        [1.88779284929066E-05,	0.00107604192409567,	0,	0.202648740695406],	
        [1.74083281691501E-05,	0.00108801726516137,	0,	0.197786565407521],	
        [1.83342941704071E-05,	0.00103523720308972,	0,	0.206072232928399],	
        [1.79805968552292E-05,	0.00101246312744667,	0,	0.201677235874478],	
        [1.87338393571272E-05,	0.00105539795005486,	0,	0.191491609860651],	
        [1.77625110078364E-05,	0.00102489402074806,	0,	0.203673288607077],	
        [1.85157535097343E-05,	0.00106782884335625,	0,	0.1953876210862],	
        [1.81620561945565E-05,	0.00104505476771321,	0,	0.199782618140121],	
        [1.90880221958135E-05,	0.000992274705641554,	0,	0.209968244153949],	
        [1.72284314594407E-05,	0.00104259279609076,	0,	0.196054394504036],	
        [1.82481751824818E-05,	0.000839416058394161,	0,	0.208521897810219],	
        [1.78789289688708E-05,	0.000856401384220264,	0,	0.212741274293151],	
        [1.86174213960927E-05,	0.000822430732568055,	0,	0.204302521327286],	
        [1.76184218720569E-05,	0.000847440188362605,	0,	0.215718088888443],	
        [1.84226127904914E-05,	0.000810447406114619,	0,	0.206528599263492],	
        [1.80737375744721E-05,	0.000831391928425715,	0,	0.201325706731995],	
        [1.88779284929066E-05,	0.000868384710673702,	0,	0.210515196356945],	
        [1.74083281691501E-05,	0.000878049021007421,	0,	0.205464280264704],	
        [1.83342941704071E-05,	0.000835454584949598,	0,	0.214071582333892],	
        [1.79805968552292E-05,	0.000817075506360474,	0,	0.209505979485242],	
        [1.87338393571272E-05,	0.000851724661447778,	0,	0.198924965988878],	
        [1.77625110078364E-05,	0.000827107455340543,	0,	0.211579515355734],	
        [1.85157535097343E-05,	0.000861756610427851,	0,	0.202972213286546],	
        [1.81620561945565E-05,	0.000843377531838727,	0,	0.207537816135197],	
        [1.90880221958135E-05,	0.000800783095780903,	0,	0.218118829631561],	
        [1.72284314594407E-05,	0.000841390677546926,	0,	0.203664869636147]])	

        
        #outputs of each layer using the validation data set 
        resulth1 = sigmoid(np.dot(validation_point, wh1))   #outputs of the input layer
        resulto1 = sigmoid(np.dot(resulth1, wh2))           #outputs of the 1st hidden layer
        resulto2 = sigmoid(np.dot(resulto1, wo))           #outputs of the 2nd hidden layer

        #experimental data for the validation set
        experimental_results = np.array([[0.014,	0.029,	0,	0,	0.854,	0.054,	0,	0],
        [0.0137167143049177,	0.0295868060826705,	0,	0,	0.836719572599977,	0.0550926733953175,	0,	0],
        [0.0142832856950823,	0.0284131939173294,	0,	0,	0.871280427400022,	0.0529073266046824,	0,	0],
        [0.0135168532602421,	0.029277216246649,	0,	0,	0.862163540504766,	0.0521364340037909,	0,	0],
        [0.014133828532865,	0.0279991960390729,	0,	0,	0.824528048874767,	0.0545161957696222,	0,	0],
        [0.013866171467135,	0.028722783753351,	0,	0,	0.883471951125233,	0.0558635659962091,	0,	0],
        [0.0144831467397579,	0.0300008039609271,	0,	0,	0.845836459495233,	0.0534838042303777,	0,	0],
        [0.0133556693713719,	0.0303346848735868,	0,	0,	0.831271305157937,	0.0515147247181489,	0,	0],
        [0.0140660704875363,	0.0288631397043892,	0,	0,	0.866522451652758,	0.0542548433090687,	0,	0],
        [0.0137947139073318,	0.0282281824936536,	0,	0,	0.849969700260288,	0.0554371774256108,	0,	0],
        [0.014372601554788,	0.0294252354776698,	0,	0,	0.893304168346315,	0.0532081822139942,	0,	0],
        [0.0136273984452121,	0.0285747645223302,	0,	0,	0.858030299739717,	0.0525628225743894,	0,	0],
        [0.0142052860926682,	0.0297718175063465,	0,	0,	0.814695831653688,	0.0547918177860058,	0,	0],
        [0.0139339295124637,	0.029136860295611,	0,	0,	0.876728694842067,	0.0564852752818513,	0,	0],
        [0.0146443306286281,	0.0276653151264133,	0,	0,	0.841477548347242,	0.0537451566909315,	0,	0],
        [0.0132176526156829,	0.0290682187990777,	0,	0,	0.851991073985778,	0.053061805897146,	0,	0],
        [0.011,	0.047,	0,	0,	0.814,	0.069,	0,	0],
        [0.0107774183824353,	0.0479510305477764,	0,	0,	0.797528960300212,	0.0703961937829057,	0,	0],
        [0.0112225816175647,	0.0460489694522235,	0,	0,	0.830471039699786,	0.0676038062170942,	0,	0],
        [0.0106203847044759,	0.0474492815031897,	0,	0,	0.821781173268009,	0.0666187767826217,	0,	0],
        [0.0111051509901082,	0.0453780073736698,	0,	0,	0.785908468131218,	0.0696595834834062,	0,	0],
        [0.0108948490098918,	0.0465507184968102,	0,	0,	0.842091531868782,	0.0713812232173783,	0,	0],
        [0.0113796152955241,	0.0486219926263302,	0,	0,	0.80621882673199,	0.0683404165165938,	0,	0],
        [0.0104937402203637,	0.0491631099675373,	0,	0,	0.792335881028759,	0.0658243704731903,	0,	0],
        [0.0110519125259214,	0.0467781919346997,	0,	0,	0.82593591995942,	0.0693256331171434,	0,	0],
        [0.0108387037843322,	0.0457491233517834,	0,	0,	0.81015847308182,	0.0708363933771693,	0,	0],
        [0.0112927583644763,	0.0476891747396717,	0,	0,	0.851463223693092,	0.0679882328289926,	0,	0],
        [0.0107072416355238,	0.0463108252603283,	0,	0,	0.817841526918184,	0.067163606622831,	0,	0],
        [0.0111612962156678,	0.0482508766482168,	0,	0,	0.776536776306911,	0.0700117671710074,	0,	0],
        [0.0109480874740786,	0.0472218080653006,	0,	0,	0.835664118971244,	0.07217562952681,	0,	0],
        [0.0115062597796364,	0.0448368900324629,	0,	0,	0.80206408004058,	0.068674366882857,	0,	0],
        [0.0103852984837509,	0.0471105615019535,	0,	0,	0.812085168881058,	0.067801196424131,	0,	0],
        [0,	0.034,	0,	0,	0.842,	0.059,	0,	0],
        [0,	0.0346879795451999,	0,	0,	0.824962388910047,	0.0601938468578469,	0,	0],
        [0,	0.0333120204548,	0,	0,	0.859037611089951,	0.057806153142153,	0,	0],
        [0,	0.0343250121512436,	0,	0,	0.850048830333739,	0.0569638815967345,	0,	0],
        [0,	0.0328266436320165,	0,	0,	0.812942174651702,	0.0595639916742169,	0,	0],
        [0,	0.0336749878487563,	0,	0,	0.871057825348298,	0.0610361184032655,	0,	0],
        [0,	0.0351733563679835,	0,	0,	0.83395116966626,	0.0584360083257831,	0,	0],
        [0,	0.0355648029552397,	0,	0,	0.819590677919183,	0.056284606636496,	0,	0],
        [0,	0.0338395431016976,	0,	0,	0.854346492144757,	0.0592784399117603,	0,	0],
        [0,	0.0330951105098008,	0,	0,	0.838026332106747,	0.0605702494094636,	0,	0],
        [0,	0.034498551939337,	0,	0,	0.880751884950349,	0.058134865752327,	0,	0],
        [0,	0.033501448060663,	0,	0,	0.845973667893257,	0.0574297505905366,	0,	0],
        [0,	0.0349048894901994,	0,	0,	0.803248115049655,	0.059865134247673,	0,	0],
        [0,	0.0341604568983025,	0,	0,	0.86440932208082,	0.0617153933635042,	0,	0],
        [0,	0.0324351970447604,	0,	0,	0.829653507855244,	0.05872156008824,	0,	0],
        [0,	0.0340799806609877,	0,	0,	0.840019302454362,	0.0579749360728077,	0,	0]])

        #defining the error tables
        error_table = abs(resulto2 - experimental_results)
        
        error[i-1,j-1] = error_table.sum()

#Finding the optimum number of neurons in each hidden layer accodring to the objective function
for i in range(0,max_nodes):
    for j in range(0,max_nodes):
        if error[i,j] == error.min():
            x = i
            y = j

print ("The minimum error is found in the following coordinates (number of nodes): x=", x, ", y=", y)
print ("The error of the optimized NN is: ", error[x,y])
print ("The optimum nodes combination therefore is: HL1: ", x+1, ", HL2: ", y+1)


######Prediction experiment (quadraple knockout)
np.random.seed(0)

#ANN parameters
wh1 = np.random.rand(len(feature_set[0]),x+1)
wh2 = np.random.rand(x+1, y+1)
wo  = np.random.rand(y+1, len(labels[0]))
lr = 0.5

#print (wh1) #it was silenced in order to save some time during optimization

for epoch in range(epochs):
    #FEEDFORWARD
    zh1 = np.dot(feature_set, wh1)
    ah1 = sigmoid(zh1)

    zh2 = np.dot(ah1, wh2)
    ah2 = sigmoid(zh2)
    
    zo = np.dot(ah2, wo)
    ao = sigmoid(zo)

#ANN error
    error_output = ((1/2)*(np.power((ao - labels), 2)))

#Part1: from HL2 to the Output
    dcost_dao = ao - labels
    dao_dzo   = sigmoid_der(zo)
    dzo_dwo   = ah2
    dcost_dwo = np.dot(dzo_dwo.T, dcost_dao*dao_dzo)
#Part2: from HL1 to HL2
    dcost_dzo = dcost_dao*dao_dzo
    dzo_dah2  = wo
    dcost_dah2 = np.dot(dcost_dzo, dzo_dah2.T)
    dah2_dzh2 = sigmoid_der(zh2)
    dzh2_dwh2 = ah1
    dcost_dwh2 = np.dot(dzh2_dwh2.T, dah2_dzh2*dcost_dah2)
#Part2: from the Input to HL1
    dcost_dzh2 = dcost_dah2*dah2_dzh2
    dzh2_dah1  = wh2
    dcost_dah1 = np.dot(dcost_dzh2, dzh2_dah1.T)
    dah1_dzh1 = sigmoid_der(zh1)
    dzh1_dwh1 = feature_set
    dcost_dwh1 = np.dot(dzh1_dwh1.T, dah1_dzh1*dcost_dah1)
#update weights
    wo  -= lr*dcost_dwo
    wh2 -= lr*dcost_dwh2
    wh1 -= lr*dcost_dwh1

predictive_point = np.array([0.0022742, 0.0085, 0.00055, 0]) #quadruple knockout

#outputs of each layer 
predh1 = sigmoid(np.dot(predictive_point, wh1)) 
predo1 = sigmoid(np.dot(predh1, wh2))
predo2 = sigmoid(np.dot(predo1, wo))

print ("The results of 4KOs prediction are: ")
print (predo2)
