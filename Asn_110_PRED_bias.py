#Prediction of the Asn110-site glycosylation distributon of Fc-DAO protein for quadruple Î²-1,4-galacosyltransferase knockouts
#using of triple-knockouts of b4GalT for training and validation
#The present file trains a network of 2 hidden layers and tunes the number of neurons in each layer
#The script is developed for Python 3.7

#HL1 = 1
#HLL2 =6
import numpy as np

#######################
#Defining the sigmoid activation function and its derivative
def sigmoid(x):
    return 1/(1+np.exp(-x))

def sigmoid_der(x):
    return sigmoid(x)*(1-sigmoid(x))
########################

max_nodes = 15
epochs = 20000

error = np.zeros([max_nodes, max_nodes]) #recording the difference between the output of the network and the experimental data

### Define the training set
for i in range(1, max_nodes+1):
    print(i)
    for j in range(1, max_nodes+1):

        np.random.seed(0)
        feature_set = np.array([[0.182481751824818,	0.182481751824818,	0.182481751824818,	0.182481751824818],
        [0.178789289688708,	0.186174213960927,	0.178789289688708,	0.186174213960927],
        [0.186174213960927,	0.178789289688708,	0.186174213960927,	0.178789289688708],
        [0.176184218720569,	0.184226127904914,	0.188779284929066,	0.188779284929066],
        [0.184226127904914,	0.176184218720569,	0.180737375744721,	0.180737375744721],
        [0.180737375744721,	0.180737375744721,	0.184226127904914,	0.176184218720569],
        [0.188779284929066,	0.188779284929066,	0.176184218720569,	0.184226127904914],
        [0.174083281691501,	0.190880221958135,	0.185157535097343,	0.179805968552292],
        [0.183342941704071,	0.181620561945565,	0.177625110078364,	0.187338393571272],
        [0.179805968552292,	0.177625110078364,	0.190880221958135,	0.183342941704071],
        [0.187338393571272,	0.185157535097343,	0.181620561945565,	0.174083281691501],
        [0.177625110078364,	0.179805968552292,	0.179805968552292,	0.185157535097343],
        [0.185157535097343,	0.187338393571272,	0.187338393571272,	0.177625110078364],
        [0.181620561945565,	0.183342941704071,	0.174083281691501,	0.181620561945565],
        [0.190880221958135,	0.174083281691501,	0.183342941704071,	0.190880221958135],
        [0.172284314594407,	0.182911016858027,	0.181183200131412,	0.178231267730942],
        [0.172080291970803,	0.000565693430656934,	3.64963503649635E-05,	0],
        [0.168598300176451,	0.000577140063278874,	3.57578579377415E-05,	0],
        [0.175562283765154,	0.000554246798034994,	3.72348427921854E-05,	0],
        [0.166141718253497,	0.000571100996505234,	3.77558569858131E-05,	0],
        [0.173725238614334,	0.000546171078033765,	3.61474751489441E-05,	0],
        [0.170435345327272,	0.000560285864808634,	3.68452255809828E-05,	0],
        [0.178018865688109,	0.000585215783280104,	3.52368437441139E-05,	0],
        [0.164160534635085,	0.000591728688070219,	3.70315070194686E-05,	0],
        [0.172892394026939,	0.000563023742031251,	3.55250220156728E-05,	0],
        [0.169557028344811,	0.000550637841242928,	0.000038176044391627,	0],
        [0.176660105137709,	0.000573988358801763,	3.63241123891129E-05,	0],
        [0.167500478803897,	0.000557398502512105,	3.59611937104584E-05,	0],
        [0.174603555596794,	0.000580749020070943,	3.74676787142544E-05,	0],
        [0.171268189914668,	0.000568363119282621,	3.48166563383001E-05,	0],
        [0.180000049306521,	0.000539658173243652,	3.66685883408142E-05,	0],
        [0.162464108662526,	0.000567024152259885,	3.62366400262823E-05,	0],
        [0.193886861313869,	0.000821167883211679,	9.12408759124088E-05,	0.00010948905109489],
        [0.189963620294252,	0.000837783962824171,	8.93946448443538E-05,	0.000111704528376556],
        [0.197810102333485,	0.000804551803599184,	9.30871069804635E-05,	0.000107273573813225],
        [0.187195732390605,	0.000829017575572114,	9.43896424645328E-05,	0.000113267570957439],
        [0.195740260898971,	0.000792828984242562,	9.03686878723603E-05,	0.000108442425446832],
        [0.192033461728766,	0.000813318190851243,	9.21130639524571E-05,	0.000105710531232342],
        [0.200577990237132,	0.000849506782180795,	8.80921093602847E-05,	0.000110535676742949],
        [0.18496348679722,	0.000858960998811608,	9.25787675486715E-05,	0.000107883581131375],
        [0.194801875560576,	0.000817292528755041,	8.88125550391819E-05,	0.000112403036142763],
        [0.19104384158681,	0.000799312995352637,	9.54401109790675E-05,	0.000110005765022443],
        [0.199047043169476,	0.000833208907938044,	9.08102809727824E-05,	0.0001044499690149],
        [0.188726679458262,	0.000809126858485314,	0.000089902984276146,	0.000111094521058406],
        [0.196729881040927,	0.000843022771070724,	0.000093669196785636,	0.000106575066047018],
        [0.192971847067163,	0.00082504323766832,	8.70416408457504E-05,	0.000108972337167339],
        [0.202810235830518,	0.000783374767611753,	9.16714708520356E-05,	0.000114528133174881],
        [0.183052084256557,	0.000823099575861123,	9.05916000657058E-05,	0.000106938760638565],
        [3.64963503649635E-05,	0.5,	1.82481751824818E-05,	0],
        [3.57578579377415E-05,	0.51011734625294,	1.78789289688708E-05,	0],
        [3.72348427921854E-05,	0.489882653747059,	1.86174213960927E-05,	0],
        [3.52368437441139E-05,	0.504779590459465,	1.88779284929066E-05,	0],
        [3.68452255809828E-05,	0.48274475929436,	1.80737375744721E-05,	0],
        [3.61474751489441E-05,	0.495220409540534,	1.84226127904914E-05,	0],
        [3.77558569858131E-05,	0.51725524070564,	1.76184218720569E-05,	0],
        [3.48166563383001E-05,	0.52301180816529,	1.85157535097343E-05,	0],
        [3.66685883408142E-05,	0.497640339730847,	1.77625110078364E-05,	0],
        [3.59611937104584E-05,	0.486692801614717,	1.90880221958135E-05,	0],
        [3.74676787142544E-05,	0.50733164616672,	1.81620561945565E-05,	0],
        [3.55250220156728E-05,	0.49266835383328,	1.79805968552292E-05,	0],
        [3.70315070194686E-05,	0.513307198385285,	1.87338393571272E-05,	0],
        [3.63241123891129E-05,	0.502359660269155,	1.74083281691501E-05,	0],
        [0.000038176044391627,	0.476988191834712,	1.83342941704071E-05,	0],
        [3.44568629188814E-05,	0.501176186190995,	1.81183200131412E-05,	0],
        [1.82481751824818E-05,	0.418266423357664,	1.82481751824818E-05,	1.82481751824818E-05],
        [1.78789289688708E-05,	0.426729915819841,	1.78789289688708E-05,	1.86174213960927E-05],
        [1.86174213960927E-05,	0.409802930895487,	1.86174213960927E-05,	1.78789289688708E-05],
        [1.76184218720569E-05,	0.422264707770854,	1.88779284929066E-05,	1.88779284929066E-05],
        [1.84226127904914E-05,	0.403831847729417,	1.80737375744721E-05,	1.80737375744721E-05],
        [1.80737375744721E-05,	0.414268138944474,	1.84226127904914E-05,	1.76184218720569E-05],
        [1.88779284929066E-05,	0.432700998985911,	1.76184218720569E-05,	1.84226127904914E-05],
        [1.74083281691501E-05,	0.437516556750241,	1.85157535097343E-05,	1.79805968552292E-05],
        [1.83342941704071E-05,	0.416292490035429,	1.77625110078364E-05,	1.87338393571272E-05],
        [1.79805968552292E-05,	0.407134514810618,	1.90880221958135E-05,	1.83342941704071E-05],
        [1.87338393571272E-05,	0.42439958619662,	1.81620561945565E-05,	1.74083281691501E-05],
        [1.77625110078364E-05,	0.412133260518708,	1.79805968552292E-05,	1.85157535097343E-05],
        [1.85157535097343E-05,	0.429398331904712,	1.87338393571272E-05,	1.77625110078364E-05],
        [1.81620561945565E-05,	0.420240356679902,	1.74083281691501E-05,	1.81620561945565E-05],
        [1.90880221958135E-05,	0.399016289965089,	1.83342941704071E-05,	1.90880221958135E-05],
        [1.72284314594407E-05,	0.419250341740285,	1.81183200131412E-05,	1.78231267730942E-05],
        [5.47445255474452E-05,	0.385547445255474,	0,	0],
        [5.36367869066123E-05,	0.393348879256647,	0,	0],
        [5.58522641882781E-05,	0.377746011254302,	0,	0],
        [5.28552656161708E-05,	0.389232963037503,	0,	0],
        [5.52678383714743E-05,	0.372242017312819,	0,	0],
        [5.42212127234162E-05,	0.381861927473446,	0,	0],
        [5.66337854787197E-05,	0.39885287319813,	0,	0],
        [5.22249845074502E-05,	0.403291732953148,	0,	0],
        [5.50028825112213E-05,	0.383727923278589,	0,	0],
        [5.39417905656876E-05,	0.375286332573567,	0,	0],
        [5.62015180713816E-05,	0.391200840153666,	0,	0],
        [5.32875330235092E-05,	0.379894050357282,	0,	0],
        [5.55472605292029E-05,	0.395808557937383,	0,	0],
        [5.44861685836694E-05,	0.387366967232362,	0,	0],
        [5.72640665874405E-05,	0.367803157557803,	0,	0],
        [5.16852943783221E-05,	0.38645439641764,	0,	0],
        [3.64963503649635E-05,	0.000291970802919708,	0.283868613138686,	0],
        [3.57578579377415E-05,	0.000297878742337483,	0.278124619039754,	0],
        [3.72348427921854E-05,	0.000286062863501932,	0.289612607237618,	0],
        [3.52368437441139E-05,	0.000294761804647863,	0.293665055635655,	0],
        [3.68452255809828E-05,	0.000281894749952911,	0.281155061708487,	0],
        [3.61474751489441E-05,	0.000289179801191553,	0.286582164568885,	0],
        [3.77558569858131E-05,	0.000302046855886505,	0.274072170641718,	0],
        [3.48166563383001E-05,	0.000305408355133016,	0.288031061597427,	0],
        [3.66685883408142E-05,	0.000290592899112904,	0.276313621237903,	0],
        [3.59611937104584E-05,	0.000284200176125382,	0.296933273278075,	0],
        [3.74676787142544E-05,	0.000296252056155749,	0.282528946162521,	0],
        [3.55250220156728E-05,	0.000287689549683667,	0.279706164679945,	0],
        [3.70315070194686E-05,	0.000299741429714035,	0.291423605039471,	0],
        [3.63241123891129E-05,	0.000293348706726514,	0.270803952999299,	0],
        [0.000038176044391627,	0.000278533250706401,	0.285208280114853,	0],
        [3.44568629188814E-05,	0.000292657626972844,	0.281848586124424,	0],
        [5.47445255474452E-05,	0.000602189781021898,	0.127700729927007,	0],
        [5.36367869066123E-05,	0.000614374906071059,	0.125116744924158,	0],
        [5.58522641882781E-05,	0.000590004655972735,	0.130284714929857,	0],
        [5.28552656161708E-05,	0.000607946222086217,	0.13210774359336,	0],
        [5.52678383714743E-05,	0.000581407921777879,	0.126480015546155,	0],
        [5.42212127234162E-05,	0.000596433339957578,	0.128921444307859,	0],
        [5.66337854787197E-05,	0.000622971640265917,	0.123293716260654,	0],
        [5.22249845074502E-05,	0.000629904732461846,	0.129573243061121,	0],
        [5.50028825112213E-05,	0.000599347854420364,	0.124302052032839,	0],
        [5.39417905656876E-05,	0.000586162863258601,	0.133577979326303,	0],
        [5.62015180713816E-05,	0.000611019865821232,	0.127098069249506,	0],
        [5.32875330235092E-05,	0.000593359696222564,	0.125828216792894,	0],
        [5.55472605292029E-05,	0.000618216698785197,	0.131099407821176,	0],
        [5.44861685836694E-05,	0.000605031707623435,	0.121823480527712,	0],
        [5.72640665874405E-05,	0.000574474829581952,	0.128303390604509,	0],
        [5.16852943783221E-05,	0.00060360635563149,	0.126792003451962,	0],
        [0.000164233576642336,	0.00295620437956204,	0.30742700729927,	0],
        [0.000160910360719837,	0.00301602226616702,	0.301206316338566,	0],
        [0.000167556792564834,	0.00289638649295706,	0.313647698259974,	0],
        [0.000158565796848512,	0.00298446327205961,	0.318036461319997,	0],
        [0.000165803515114423,	0.00285418434327322,	0.304488256917131,	0],
        [0.000162663638170249,	0.00292794548706447,	0.310365757681409,	0],
        [0.000169901356436159,	0.00305822441585086,	0.296817553278543,	0],
        [0.000156674953522351,	0.00309225959572179,	0.311934899378494,	0],
        [0.000165008647533664,	0.00294225310351815,	0.29924502294902,	0],
        [0.000161825371697063,	0.00287752678326949,	0.32157590993287,	0],
        [0.000168604554214145,	0.00299955206857696,	0.305976160709693,	0],
        [0.000159862599070527,	0.00291285669054713,	0.302919115220046,	0],
        [0.000166641781587609,	0.0030348819758546,	0.315608991649522,	0],
        [0.000163458505751008,	0.00297015565560595,	0.293278104665671,	0],
        [0.000171792199762322,	0.00282014916340231,	0.308877853888849,	0],
        [0.000155055883134966,	0.00296315847310004,	0.305239337261389,	0]])

        
        labels = np.array([[0.143,	0.63,	0.157,	0,	0.014,	0.017,	0.04],
        [0.140106438971659,	0.642747856278704,	0.153823153276577,	0,	0.0137167143049177,	0.0173439897726,	0.0391906122997647],
        [0.145893561028341,	0.617252143721294,	0.160176846723423,	0,	0.0142832856950823,	0.0166560102274,	0.0408093877002352],
        [0.138065001158187,	0.636022283978926,	0.162418145581571,	0,	0.014133828532865,	0.0164133218160082,	0.0396176327632428],
        [0.144366962871407,	0.608258396710894,	0.155499208595728,	0,	0.0135168532602421,	0.0171625060756218,	0.0413804192564512],
        [0.141633037128593,	0.623977716021073,	0.158500791404272,	0,	0.0144831467397579,	0.0175866781839918,	0.0386195807435488],
        [0.147934998841813,	0.651741603289106,	0.151581854418429,	0,	0.013866171467135,	0.0168374939243782,	0.0403823672367572],
        [0.136418622864728,	0.658994878288265,	0.15930213689635,	0,	0.0136273984452121,	0.0162175985223802,	0.0398112271784678],
        [0.143674862836978,	0.627026828060868,	0.152821539707021,	0,	0.0142052860926682,	0.0170802284491513,	0.0418409446532232],
        [0.140903149196318,	0.613232930034543,	0.164225707763901,	0,	0.0139339295124637,	0.0174524447450997,	0.0389354241291774],
        [0.146805858738192,	0.639237874170067,	0.156259066675486,	0,	0.0146443306286281,	0.0167507240303315,	0.0405865316933376],
        [0.139194141261809,	0.620762125829933,	0.15469786310365,	0,	0.0140660704875363,	0.0165475552549004,	0.038159055346777],
        [0.145096850803682,	0.646767069965459,	0.161178460292979,	0,	0.0133556693713719,	0.0172492759696685,	0.0401887728215324],
        [0.142325137163022,	0.632973171939135,	0.1497742922361,	0,	0.014372601554788,	0.0177824014776199,	0.0394134683066624],
        [0.149581377135273,	0.601005121711737,	0.157740933324515,	0,	0.0137947139073318,	0.0169197715508488,	0.0410645758708228],
        [0.135008880288761,	0.631481994600654,	0.155882778065061,	0,	0.0139670667866521,	0.0167046425972497,	0.042235278240906],
        [0.138,	0.621,	0.166,	0,	0.018,	0.019,	0.039],
        [0.135207612434188,	0.633565744046152,	0.162641041044024,	0,	0.0176357755348941,	0.0193844591576117,	0.0382108469922706],
        [0.140792387565811,	0.608434255953847,	0.169358958955976,	0,	0.0183642244651058,	0.0186155408423882,	0.0397891530077293],
        [0.133237553565243,	0.626936251350655,	0.171728739914272,	0,	0.0181720652565407,	0.0183443008531857,	0.0386271919441617],
        [0.139319166966812,	0.599568991043595,	0.164413175967457,	0,	0.017378811334597,	0.0191816244374597,	0.0403459087750399],
        [0.136680833033188,	0.615063748649344,	0.167586824032542,	0,	0.018621188665403,	0.0196556991468143,	0.0376540912249601],
        [0.142762446434757,	0.642431008956405,	0.160271260085728,	0,	0.0178279347434592,	0.0188183755625403,	0.0393728080558383],
        [0.131648740946381,	0.64958066574129,	0.168434106527351,	0,	0.0175209408581298,	0.0181255512897191,	0.0388159464990061],
        [0.138651266234287,	0.618069301945713,	0.161582010136086,	0,	0.0182639392620019,	0.0190896670902279,	0.0407949210368926],
        [0.135976465657985,	0.604472459605479,	0.173639920310876,	0,	0.0179150522303105,	0.0195056735386408,	0.0379620385259479],
        [0.141672786754339,	0.630105904539066,	0.165216592790641,	0,	0.0188284250939504,	0.0187213974456646,	0.0395718684010042],
        [0.134327213245662,	0.611894095460934,	0.163565893472649,	0,	0.0180849477696896,	0.0184943264613592,	0.0372050789631075],
        [0.140023534342015,	0.637527540394524,	0.170417989863915,	0,	0.0171715749060496,	0.0192786025543354,	0.0391840535009941],
        [0.137348733765714,	0.623930698054291,	0.158360079689124,	0,	0.0184790591418703,	0.019874448710281,	0.0384281315989958],
        [0.14435125905362,	0.592419334258712,	0.166783407209359,	0,	0.0177360607379981,	0.0189103329097722,	0.0400379614740522],
        [0.130288290068874,	0.622460823249216,	0.164818733495543,	0,	0.0179576572971241,	0.0186698946675143,	0.0411793962848834],
        [0.1,	0.6,	0.258,	0,	0,	0.014,	0.028],
        [0.0979765307494118,	0.612140815503528,	0.252779449333482,	0,	0,	0.0142832856950823,	0.0274334286098353],
        [0.102023469250588,	0.587859184496471,	0.263220550666517,	0,	0,	0.0137167143049177,	0.0285665713901646],
        [0.096548951858872,	0.605735508551358,	0.26690370420411,	0,	0,	0.0135168532602421,	0.0277323429342699],
        [0.100955918091893,	0.579293711153232,	0.255533731322916,	0,	0,	0.014133828532865,	0.0289662934795158],
        [0.0990440819081069,	0.594264491448641,	0.260466268677084,	0,	0,	0.0144831467397579,	0.0270337065204842],
        [0.103451048141128,	0.620706288846768,	0.24909629579589,	0,	0,	0.013866171467135,	0.02826765706573],
        [0.0953976383669424,	0.627614169798348,	0.261783129422028,	0,	0,	0.0133556693713719,	0.0278678590249275],
        [0.100471932053831,	0.597168407677017,	0.251133485633194,	0,	0,	0.0140660704875363,	0.0292886612572562],
        [0.098533670766656,	0.58403136193766,	0.26987409301329,	0,	0,	0.014372601554788,	0.0272547968904241],
        [0.102661439677057,	0.608797975400064,	0.256782415301117,	0,	0,	0.0137947139073318,	0.0284105721853363],
        [0.0973385603229434,	0.591202024599936,	0.254216870577972,	0,	0,	0.0136273984452121,	0.0267113387427439],
        [0.101466329233344,	0.615968638062342,	0.264866514366807,	0,	0,	0.0142052860926682,	0.0281321409750727],
        [0.0995280679461695,	0.602831592322986,	0.246125906986711,	0,	0,	0.0146443306286281,	0.0275894278146637],
        [0.104602361633058,	0.572385830201654,	0.259217584698884,	0,	0,	0.0139339295124637,	0.028745203109576],
        [0.094411804397735,	0.601411423429194,	0.256164055673795,	0,	0,	0.0137567644918527,	0.0295646947686342],
        [0.187,	0.63,	0.144,	0.027,	0,	0,	0.011],
        [0.1832161125014,	0.642747856278704,	0.141086204279153,	0.0275463366976588,	0,	0,	0.0107774183824353],
        [0.1907838874986,	0.617252143721294,	0.146913795720847,	0.0264536633023412,	0,	0,	0.0112225816175647],
        [0.180546539976091,	0.636022283978926,	0.148969509323224,	0.0279317829981046,	0,	0,	0.0108948490098918],
        [0.18878756683184,	0.608258396710894,	0.142623477947674,	0.0267419021151889,	0,	0,	0.0113796152955241],
        [0.18521243316816,	0.623977716021073,	0.145376522052326,	0.0260682170018954,	0,	0,	0.0106203847044759],
        [0.193453460023909,	0.651741603289106,	0.139030490676776,	0.0272580978848111,	0,	0,	0.0111051509901082],
        [0.178393583746182,	0.658994878288265,	0.146111514096015,	0.0266040911069971,	0,	0,	0.0109480874740786],
        [0.187882512940664,	0.627026828060868,	0.140167526865039,	0.0277185887128054,	0,	0,	0.0115062597796364],
        [0.184257964333647,	0.613232930034543,	0.150627400751604,	0.0271274216545344,	0,	0,	0.0107072416355238],
        [0.191976892196097,	0.639237874170067,	0.143320417842484,	0.0257573623590745,	0,	0,	0.0111612962156678],
        [0.182023107803904,	0.620762125829933,	0.141888485903985,	0.0273959088930029,	0,	0,	0.0104937402203637],
        [0.189742035666353,	0.646767069965459,	0.147832473134962,	0.0262814112871947,	0,	0,	0.0110519125259214],
        [0.186117487059337,	0.632973171939135,	0.137372599248397,	0.0268725783454658,	0,	0,	0.0108387037843322],
        [0.195606416253818,	0.601005121711737,	0.144679582157517,	0.0282426376409257,	0,	0,	0.0112927583644763],
        [0.176550074223764,	0.631481994600654,	0.1429752868877,	0.0263710983734702,	0,	0,	0.0116147015162492],
        [0.193,	0.635,	0.142,	0.018,	0,	0,	0.011],
        [0.189094704346365,	0.647849029741234,	0.139126673664165,	0.0183642244651058,	0,	0,	0.0107774183824353],
        [0.196905295653635,	0.622150970258765,	0.144873326335835,	0.0176357755348941,	0,	0,	0.0112225816175647],
        [0.186339477087623,	0.64107007988352,	0.146900488360402,	0.018621188665403,	0,	0,	0.0108948490098918],
        [0.194844921917353,	0.613085844303837,	0.140642596309512,	0.0178279347434592,	0,	0,	0.0113796152955241],
        [0.191155078082646,	0.628929920116479,	0.143357403690488,	0.017378811334597,	0,	0,	0.0106203847044759],
        [0.199660522912377,	0.656914155696163,	0.137099511639598,	0.0181720652565407,	0,	0,	0.0111051509901082],
        [0.184117442048199,	0.664224996369918,	0.144082187511348,	0.0177360607379981,	0,	0,	0.0109480874740786],
        [0.193910828863894,	0.632003231458176,	0.13822075565858,	0.0184790591418703,	0,	0,	0.0115062597796364],
        [0.190169984579646,	0.618099858050691,	0.148535353518942,	0.0180849477696896,	0,	0,	0.0107072416355238],
        [0.19813657857672,	0.644311190631734,	0.141329856483561,	0.0171715749060496,	0,	0,	0.0111612962156678],
        [0.187863421423281,	0.625688809368266,	0.139917812488652,	0.0182639392620019,	0,	0,	0.0104937402203637],
        [0.195830015420354,	0.651900141949312,	0.145779244341421,	0.0175209408581298,	0,	0,	0.0110519125259214],
        [0.192089171136107,	0.637996768541827,	0.135464646481058,	0.0179150522303105,	0,	0,	0.0108387037843322],
        [0.201882557951802,	0.605775003630084,	0.14267014351644,	0.0188284250939504,	0,	0,	0.0112927583644763],
        [0.182214782487629,	0.636493756462564,	0.140989519014259,	0.0175807322489801,	0,	0,	0.0116147015162492],
        [0.185,	0.653,	0.143,	0.02,	0,	0,	0],
        [0.181256581886412,	0.66621325420634,	0.140106438971659,	0.0204046938501176,	0,	0,	0],
        [0.188743418113588,	0.639786745793659,	0.145893561028341,	0.0195953061498824,	0,	0,	0],
        [0.178615560938913,	0.659242145140061,	0.147934998841813,	0.0206902096282256,	0,	0,	0],
        [0.186768448470002,	0.630464655638434,	0.141633037128593,	0.0198088163816214,	0,	0,	0],
        [0.183231551529998,	0.646757854859938,	0.144366962871407,	0.0193097903717744,	0,	0,	0],
        [0.191384439061087,	0.675535344361566,	0.138065001158187,	0.0201911836183786,	0,	0,	0],
        [0.176485630978843,	0.683053421463869,	0.145096850803682,	0.0197067341533312,	0,	0,	0],
        [0.185873074299587,	0.649918283688487,	0.139194141261809,	0.0205322879354114,	0,	0,	0],
        [0.182287290918314,	0.63562079890882,	0.149581377135273,	0.0200943864107662,	0,	0,	0],
        [0.189923663402555,	0.662575129893736,	0.142325137163022,	0.0190795276733885,	0,	0,	0],
        [0.180076336597445,	0.643424870106264,	0.140903149196318,	0.0202932658466688,	0,	0,	0],
        [0.187712709081686,	0.670379201091182,	0.146805858738192,	0.0194677120645887,	0,	0,	0],
        [0.184126925700414,	0.656081716311516,	0.136418622864728,	0.0199056135892339,	0,	0,	0],
        [0.193514369021157,	0.622946578536134,	0.143674862836978,	0.0209204723266116,	0,	0,	0],
        [0.17466183813581,	0.65453609916544,	0.141982402950979,	0.0195341469433112,	0,	0,	0],
        [0.186,	0.628,	0.155,	0.032,	0,	0,	0],
        [0.182236347193906,	0.640707386893693,	0.151863622661588,	0.0326475101601882,	0,	0,	0],
        [0.189763652806094,	0.615292613106306,	0.158136377338411,	0.0313524898398118,	0,	0,	0],
        [0.179581050457502,	0.634003165617088,	0.160349124618748,	0.033104335405161,	0,	0,	0],
        [0.187778007650921,	0.606327417673716,	0.153518326957566,	0.0316941062105942,	0,	0,	0],
        [0.184221992349079,	0.621996834382911,	0.156481673042434,	0.030895664594839,	0,	0,	0],
        [0.192418949542498,	0.649672582326284,	0.149650875381252,	0.0323058937894058,	0,	0,	0],
        [0.177439607362513,	0.656902831055604,	0.157272810311683,	0.0315307746453299,	0,	0,	0],
        [0.186877793620126,	0.625036266701944,	0.150874768500562,	0.0328516606966582,	0,	0,	0],
        [0.18327262762598,	0.611286158828085,	0.16213366053124,	0.0321510182572259,	0,	0,	0],
        [0.190950277799326,	0.6372085475854,	0.154268505316563,	0.0305272442774216,	0,	0,	0],
        [0.181049722200675,	0.6187914524146,	0.152727189688317,	0.0324692253546701,	0,	0,	0],
        [0.18872737237402,	0.644713841171918,	0.159125231499438,	0.0311483393033419,	0,	0,	0],
        [0.185122206379875,	0.630963733298059,	0.147866339468761,	0.0318489817427742,	0,	0,	0],
        [0.194560392637488,	0.599097168944398,	0.155731494683438,	0.0334727557225786,	0,	0,	0],
        [0.175605956179787,	0.62947728985589,	0.153897010191621,	0.031254635109298,	0,	0,	0],
        [0.177,	0.63,	0.147,	0.046,	0,	0,	0],
        [0.173418459426459,	0.642747856278704,	0.144025500201635,	0.0469307958552705,	0,	0,	0],
        [0.180581540573541,	0.617252143721294,	0.149974499798364,	0.0450692041447294,	0,	0,	0],
        [0.170891644790203,	0.636022283978926,	0.152073040767458,	0.0475874821449189,	0,	0,	0],
        [0.178691975022651,	0.608258396710894,	0.145594800404917,	0.0455602776777292,	0,	0,	0],
        [0.175308024977349,	0.623977716021073,	0.148405199595083,	0.0444125178550811,	0,	0,	0],
        [0.183108355209797,	0.651741603289106,	0.141926959232542,	0.0464397223222708,	0,	0,	0],
        [0.168853819909488,	0.658994878288265,	0.149155503973016,	0.0453254885526618,	0,	0,	0],
        [0.177835319735281,	0.627026828060868,	0.143087683674727,	0.0472242622514462,	0,	0,	0],
        [0.174404597256981,	0.613232930034543,	0.153765471600595,	0.0462170887447623,	0,	0,	0],
        [0.181710748228391,	0.639237874170067,	0.146306259880869,	0.0438829136487935,	0,	0,	0],
        [0.17228925177161,	0.620762125829933,	0.144844496026984,	0.0466745114473382,	0,	0,	0],
        [0.179595402743019,	0.646767069965459,	0.150912316325274,	0.044775737748554,	0,	0,	0],
        [0.17616468026472,	0.632973171939135,	0.140234528399405,	0.045782911255238,	0,	0,	0],
        [0.185146180090513,	0.601005121711737,	0.147693740119132,	0.0481170863512067,	0,	0,	0],
        [0.167108893783991,	0.631481994600654,	0.14595393869786,	0.0449285379696159,	0,	0,	0],
        [0.157,	0.62,	0.184,	0.039,	0,	0,	0],
        [0.153823153276577,	0.632545509353646,	0.180276816578918,	0.0397891530077293,	0,	0,	0],
        [0.160176846723423,	0.607454490646353,	0.187723183421082,	0.0382108469922706,	0,	0,	0],
        [0.151581854418429,	0.625926692169737,	0.190349928579676,	0.0403459087750399,	0,	0,	0],
        [0.158500791404272,	0.598603501525006,	0.182241110710917,	0.0386271919441617,	0,	0,	0],
        [0.155499208595728,	0.614073307830263,	0.185758889289083,	0.0376540912249601,	0,	0,	0],
        [0.162418145581571,	0.641396498474994,	0.177650071420324,	0.0393728080558383,	0,	0,	0],
        [0.1497742922361,	0.64853464212496,	0.186698045789353,	0.0384281315989958,	0,	0,	0],
        [0.157740933324515,	0.617074021266251,	0.179102950994216,	0.0400379614740522,	0,	0,	0],
        [0.15469786310365,	0.603499074002249,	0.192468345404827,	0.0391840535009941,	0,	0,	0],
        [0.161178460292979,	0.629091241246733,	0.183131645020952,	0.0372050789631075,	0,	0,	0],
        [0.152821539707021,	0.610908758753267,	0.181301954210647,	0.0395718684010042,	0,	0,	0],
        [0.15930213689635,	0.636500925997753,	0.188897049005785,	0.0379620385259479,	0,	0,	0],
        [0.156259066675486,	0.622925978733752,	0.175531654595174,	0.0388159464990061,	0,	0,	0],
        [0.164225707763901,	0.591465357875043,	0.184868354979049,	0.0407949210368926,	0,	0,	0],
        [0.148226532904444,	0.621458470876834,	0.182690644356505,	0.0380915865394569,	0,	0,	0]])

        
#ANN parameters
        wh1 = np.random.rand(len(feature_set[0]),i)
        wh2 = np.random.rand(i, j)
        wo  = np.random.rand(j, len(labels[0]))
        b1  = np.random.rand(i)
        b2  = np.random.rand(j)
        b3  = np.random.rand(len(labels[0]))
        lr = 0.5

        #print (wh1) #it was silenced in order to save some time during optimization

        for epoch in range(epochs):
            #FEEDFORWARD
            zh1 = np.dot(feature_set, wh1) + b1
            ah1 = sigmoid(zh1)

            zh2 = np.dot(ah1, wh2) + b2
            ah2 = sigmoid(zh2)
            
            zo = np.dot(ah2, wo) + b3
            ao = sigmoid(zo)

#ANN error
            error_output = ((1/2)*(np.power((ao - labels), 2)))

#Part1: from HL2 to the Output
            dcost_dao = ao - labels
            dao_dzo   = sigmoid_der(zo)
            dzo_dwo   = ah2
            dcost_dwo = np.dot(dzo_dwo.T, dcost_dao*dao_dzo)
            dcost_db3 = dcost_dao*dao_dzo
#Part2: from HL1 to HL2
            dcost_dzo = dcost_dao*dao_dzo
            dzo_dah2  = wo
            dcost_dah2 = np.dot(dcost_dzo, dzo_dah2.T)
            dah2_dzh2 = sigmoid_der(zh2)
            dzh2_dwh2 = ah1
            dcost_dwh2 = np.dot(dzh2_dwh2.T, dah2_dzh2*dcost_dah2)
            dcost_db2 = dah2_dzh2*dcost_dah2
#Part2: from the Input to HL1
            dcost_dzh2 = dcost_dah2*dah2_dzh2
            dzh2_dah1  = wh2
            dcost_dah1 = np.dot(dcost_dzh2, dzh2_dah1.T)
            dah1_dzh1 = sigmoid_der(zh1)
            dzh1_dwh1 = feature_set
            dcost_dwh1 = np.dot(dzh1_dwh1.T, dah1_dzh1*dcost_dah1)
            dcost_db1 = dah1_dzh1*dcost_dah1
#update weights
            wo  -= lr*dcost_dwo
            wh2 -= lr*dcost_dwh2
            wh1 -= lr*dcost_dwh1
            for number3 in dcost_db3:
                b3  -= lr*number3
            for number2 in dcost_db2:
                b2  -= lr*number2
            for number1 in dcost_db1:
                b1  -= lr*number1

        #Validation set (tuning of the hyperparameters)
        validation_point = np.array([[1.82481751824818E-05,	0.00114963503649635,	0,	0.127919708029197],	
        [1.78789289688708E-05,	0.00117289754795384,	0,	0.13050812398661],	
        [1.86174213960927E-05,	0.00112637252503886,	0,	0.125331292071784],	
        [1.76184218720569E-05,	0.00116062460580096,	0,	0.132334278735275],	
        [1.84226127904914E-05,	0.00110996057793959,	0,	0.126696900397049],	
        [1.80737375744721E-05,	0.00113864546719174,	0,	0.123505137323119],	
        [1.88779284929066E-05,	0.00118930949505311,	0,	0.129142515661345],	
        [1.74083281691501E-05,	0.00120254539833625,	0,	0.126043983955157],	
        [1.83342941704071E-05,	0.00114420954025706,	0,	0.131324213893462],	
        [1.79805968552292E-05,	0.00111903819349369,	0,	0.128523402134554],	
        [1.87338393571272E-05,	0.00116649247111326,	0,	0.122032380465742],	
        [1.77625110078364E-05,	0.00113277760187944,	0,	0.129795432103237],	
        [1.85157535097343E-05,	0.00118023187949901,	0,	0.124515202164933],	
        [1.81620561945565E-05,	0.00115506053273565,	0,	0.127316013923841],	
        [1.90880221958135E-05,	0.00109672467465645,	0,	0.133807035592653],	
        [1.72284314594407E-05,	0.00115233940620557,	0,	0.12494011867939],	
        [1.82481751824818E-05,	0.00104014598540146,	0,	0.200729927007299],	
        [1.78789289688708E-05,	0.00106119301957728,	0,	0.20479163535702],	
        [1.86174213960927E-05,	0.00101909895122563,	0,	0.196668218657578],	
        [1.76184218720569E-05,	0.00105008892905801,	0,	0.207657213421972],	
        [1.84226127904914E-05,	0.00100425004670725,	0,	0.198811113319193],	
        [1.80737375744721E-05,	0.00103020304174491,	0,	0.193802640592626],	
        [1.88779284929066E-05,	0.00107604192409567,	0,	0.202648740695406],	
        [1.74083281691501E-05,	0.00108801726516137,	0,	0.197786565407521],	
        [1.83342941704071E-05,	0.00103523720308972,	0,	0.206072232928399],	
        [1.79805968552292E-05,	0.00101246312744667,	0,	0.201677235874478],	
        [1.87338393571272E-05,	0.00105539795005486,	0,	0.191491609860651],	
        [1.77625110078364E-05,	0.00102489402074806,	0,	0.203673288607077],	
        [1.85157535097343E-05,	0.00106782884335625,	0,	0.1953876210862],	
        [1.81620561945565E-05,	0.00104505476771321,	0,	0.199782618140121],	
        [1.90880221958135E-05,	0.000992274705641554,	0,	0.209968244153949],	
        [1.72284314594407E-05,	0.00104259279609076,	0,	0.196054394504036],	
        [1.82481751824818E-05,	0.000839416058394161,	0,	0.208521897810219],	
        [1.78789289688708E-05,	0.000856401384220264,	0,	0.212741274293151],	
        [1.86174213960927E-05,	0.000822430732568055,	0,	0.204302521327286],	
        [1.76184218720569E-05,	0.000847440188362605,	0,	0.215718088888443],	
        [1.84226127904914E-05,	0.000810447406114619,	0,	0.206528599263492],	
        [1.80737375744721E-05,	0.000831391928425715,	0,	0.201325706731995],	
        [1.88779284929066E-05,	0.000868384710673702,	0,	0.210515196356945],	
        [1.74083281691501E-05,	0.000878049021007421,	0,	0.205464280264704],	
        [1.83342941704071E-05,	0.000835454584949598,	0,	0.214071582333892],	
        [1.79805968552292E-05,	0.000817075506360474,	0,	0.209505979485242],	
        [1.87338393571272E-05,	0.000851724661447778,	0,	0.198924965988878],	
        [1.77625110078364E-05,	0.000827107455340543,	0,	0.211579515355734],	
        [1.85157535097343E-05,	0.000861756610427851,	0,	0.202972213286546],	
        [1.81620561945565E-05,	0.000843377531838727,	0,	0.207537816135197],	
        [1.90880221958135E-05,	0.000800783095780903,	0,	0.218118829631561],	
        [1.72284314594407E-05,	0.000841390677546926,	0,	0.203664869636147]])	

        
        #outputs of each layer using the validation data set 
        resulth1 = sigmoid(np.dot(validation_point, wh1)+b1)   #outputs of the input layer
        resulto1 = sigmoid(np.dot(resulth1, wh2)+b2)           #outputs of the 1st hidden layer
        resulto2 = sigmoid(np.dot(resulto1, wo)+b3)           #outputs of the 2nd hidden layer

        #experimental data for the validation set
        experimental_results = np.array([[0.131,	0.627,	0.21,	0.032,	0,	0,	0],
        [0.128349255281729,	0.639687152201187,	0.205750714573765,	0.0326475101601882,	0,	0,	0],
        [0.13365074471827,	0.614312847798812,	0.214249285426235,	0.0313524898398118,	0,	0,	0],
        [0.126479126935122,	0.632993606436169,	0.217247201096369,	0.033104335405161,	0,	0,	0],
        [0.13225225270038,	0.605361928155127,	0.207992572007024,	0.0316941062105942,	0,	0,	0],
        [0.12974774729962,	0.62100639356383,	0.212007427992975,	0.030895664594839,	0,	0,	0],
        [0.135520873064878,	0.648638071844873,	0.202752798903631,	0.0323058937894058,	0,	0,	0],
        [0.124970906260695,	0.655856807439274,	0.213079291390022,	0.0315307746453299,	0,	0,	0],
        [0.131618230990519,	0.624040986022483,	0.204410976678181,	0.0328516606966582,	0,	0,	0],
        [0.129079108704319,	0.610312773224855,	0.219664959429422,	0.0321510182572259,	0,	0,	0],
        [0.134486485976945,	0.636193884293067,	0.209008942686956,	0.0305272442774216,	0,	0,	0],
        [0.127513514023056,	0.617806115706933,	0.206920708609978,	0.0324692253546701,	0,	0,	0],
        [0.132920891295681,	0.643687226775147,	0.21558902332182,	0.0311483393033419,	0,	0,	0],
        [0.130381769009482,	0.62995901397752,	0.200335040570579,	0.0318489817427742,	0,	0,	0],
        [0.137029093739306,	0.598143192560729,	0.210991057313045,	0.0334727557225786,	0,	0,	0],
        [0.123679463761033,	0.628474937483508,	0.208505626711229,	0.031254635109298,	0,	0,	0],
        [0.205,	0.609,	0.154,	0.031,	0,	0,	0],
        [0.200851888036294,	0.621322927736081,	0.150883857354094,	0.0316272754676823,	0,	0,	0],
        [0.209148111963705,	0.596677072263918,	0.157116142645906,	0.0303727245323177,	0,	0,	0],
        [0.197925351310688,	0.614821541179628,	0.159314614137337,	0.0320698249237497,	0,	0,	0],
        [0.206959632088381,	0.587983116820531,	0.152527886138485,	0.0307036653915131,	0,	0,	0],
        [0.203040367911619,	0.603178458820371,	0.155472113861515,	0.0299301750762503,	0,	0,	0],
        [0.212074648689312,	0.630016883179469,	0.148685385862663,	0.0312963346084868,	0,	0,	0],
        [0.195565158652232,	0.637028382345323,	0.15625814701935,	0.0305454379376634,	0,	0,	0],
        [0.205967460710354,	0.606125933792172,	0.149901382897333,	0.0318250462998877,	0,	0,	0],
        [0.201994025071645,	0.592791832366725,	0.161087636914909,	0.0311462989366876,	0,	0,	0],
        [0.210455951337967,	0.617929945031065,	0.153273224637101,	0.0295732678937521,	0,	0,	0],
        [0.199544048662034,	0.600070054968935,	0.15174185298065,	0.0314545620623366,	0,	0,	0],
        [0.208005974928355,	0.625208167633277,	0.158098617102668,	0.0301749537001125,	0,	0,	0],
        [0.204032539289647,	0.611874066207831,	0.146912363085091,	0.0308537010633125,	0,	0,	0],
        [0.214434841347769,	0.580971617654679,	0.1547267753629,	0.032426732106248,	0,	0,	0],
        [0.193544199015357,	0.610432594780632,	0.152904126254901,	0.0302779277621324,	0,	0,	0],
        [0.218,	0.623,	0.123,	0.036,	0,	0,	0],
        [0.213588837033718,	0.635606213431163,	0.120511132821777,	0.0367284489302117,	0,	0,	0],
        [0.222411162966282,	0.610393786568836,	0.125488867178223,	0.0352715510697883,	0,	0,	0],
        [0.210476715052341,	0.628955369712493,	0.127244789213587,	0.0372423773308061,	0,	0,	0],
        [0.220083901440327,	0.601499970080773,	0.121824220746971,	0.0356558694869185,	0,	0,	0],
        [0.215916098559673,	0.617044630287506,	0.124175779253028,	0.0347576226691939,	0,	0,	0],
        [0.225523284947659,	0.644500029919227,	0.118755210786413,	0.0363441305130815,	0,	0,	0],
        [0.207966851639934,	0.651672712973951,	0.124803584957013,	0.0354721214759962,	0,	0,	0],
        [0.219028811877352,	0.620059863304636,	0.11972642919722,	0.0369581182837405,	0,	0,	0],
        [0.21480340227131,	0.606419230811937,	0.128660904808661,	0.0361698955393792,	0,	0,	0],
        [0.223801938495984,	0.632135231123733,	0.122419523573788,	0.0343431498120993,	0,	0,	0],
        [0.212198061504017,	0.613864768876267,	0.121196415042987,	0.0365278785240038,	0,	0,	0],
        [0.22119659772869,	0.639580769188065,	0.12627357080278,	0.0350418817162596,	0,	0,	0],
        [0.21697118812265,	0.625940136695367,	0.117339095191339,	0.035830104460621,	0,	0,	0],
        [0.228033148360066,	0.594327287026051,	0.123580476426212,	0.0376568501879009,	0,	0,	0],
        [0.205817733587062,	0.62446552799398,	0.122124724216577,	0.0351614644979602,	0,	0,	0]])
        
        #defining the error tables
        error_table = abs(resulto2 - experimental_results)
        
        error[i-1,j-1] = error_table.sum()

#Finding the optimum number of neurons in each hidden layer accodring to the objective function
for i in range(0,max_nodes):
    for j in range(0,max_nodes):
        if error[i,j] == error.min():
            x = i
            y = j

print ("The minimum error is found in the following coordinates (number of nodes): x=", x, ", y=", y)
print ("The error of the optimized NN is: ", error[x,y])
print ("The optimum nodes combination therefore is: HL1: ", x+1, ", HL2: ", y+1)


######Prediction experiment (quadraple knockout)
np.random.seed(0)
x=4
y=7
#ANN parameters
wh1 = np.random.rand(len(feature_set[0]),x+1)
wh2 = np.random.rand(x+1, y+1)
wo  = np.random.rand(y+1, len(labels[0]))
b1  = np.random.rand(x+1)
b2  = np.random.rand(y+1)
b3  = np.random.rand(len(labels[0]))
lr = 0.5

#print (wh1) #it was silenced in order to save some time during optimization

for epoch in range(epochs):
    #FEEDFORWARD
    zh1 = np.dot(feature_set, wh1) + b1
    ah1 = sigmoid(zh1)

    zh2 = np.dot(ah1, wh2) + b2
    ah2 = sigmoid(zh2)
    
    zo = np.dot(ah2, wo) + b3
    ao = sigmoid(zo)

#ANN error
    error_output = ((1/2)*(np.power((ao - labels), 2)))

#Part1: from HL2 to the Output
    dcost_dao = ao - labels
    dao_dzo   = sigmoid_der(zo)
    dzo_dwo   = ah2
    dcost_dwo = np.dot(dzo_dwo.T, dcost_dao*dao_dzo)
    dcost_db3 = dcost_dao*dao_dzo
#Part2: from HL1 to HL2
    dcost_dzo = dcost_dao*dao_dzo
    dzo_dah2  = wo
    dcost_dah2 = np.dot(dcost_dzo, dzo_dah2.T)
    dah2_dzh2 = sigmoid_der(zh2)
    dzh2_dwh2 = ah1
    dcost_dwh2 = np.dot(dzh2_dwh2.T, dah2_dzh2*dcost_dah2)
    dcost_db2 = dah2_dzh2*dcost_dah2
#Part2: from the Input to HL1
    dcost_dzh2 = dcost_dah2*dah2_dzh2
    dzh2_dah1  = wh2
    dcost_dah1 = np.dot(dcost_dzh2, dzh2_dah1.T)
    dah1_dzh1 = sigmoid_der(zh1)
    dzh1_dwh1 = feature_set
    dcost_dwh1 = np.dot(dzh1_dwh1.T, dah1_dzh1*dcost_dah1)
    dcost_db1 = dah1_dzh1*dcost_dah1
#update weights
    wo  -= lr*dcost_dwo
    wh2 -= lr*dcost_dwh2
    wh1 -= lr*dcost_dwh1
    for number3 in dcost_db3:
        b3  -= lr*number3
    for number2 in dcost_db2:
        b2  -= lr*number2
    for number1 in dcost_db1:
        b1  -= lr*number1

predictive_point = np.array([0.0022742, 0.0085, 0.00055, 0]) #quadruple knockout

#outputs of each layer 
predh1 = sigmoid(np.dot(predictive_point, wh1)+b1) 
predo1 = sigmoid(np.dot(predh1, wh2)+b2)
predo2 = sigmoid(np.dot(predo1, wo)+b3)

print ("The results of 4KOs prediction are: ")
print (predo2)
